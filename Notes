- think about caching
- think about when one table deletes completely/or even a replica then you have to clean up everything
- think about the list operation
- heartbeat message between a request_manager node and an availability zone
- send_all() worng string &
- optimize getReplicaAddr function to include static varible to store ip and port of other replica instead of finding again and again

 
Message format :- (Between Request manager and Job manager)
query = <REPLICA-ID, vector<REPLICA-ID> followers, operation, key, value>

Message format :- (Between Partition process and Job manager)
query = <REPLICA-ID, vector<REPLICA-ID> followers, operation, key, value>


Table name - string
Partition ID = HASH-VALUE % M
REPLICA-ID = <AZ_ID, SLOT_ID>// unique across all replicas (long long)



RAFT MSG FORMAT :-
<REPLICA-ID, DEST REPLICA-ID, OPERATION, >



Tasks for Sukhomay:
    - make the avl tree in a file
    - when you integrate raft ensure that the epoll of main_socket_fd is configured for write; basically the main_socket_fd will be used for accepting the first connection from a request manager to give them a dedicated port for furether connection and for all raft communicatios; mind that the request manager has to try again and again for connecting to the job manager through this dedicated port.


Imp points:
    - DynamoDB is designed for high throughput and low latency, so it doesnâ€™t create a new TCP connection for every single request (such as a GET or SET). Instead, its request routing layer typically uses persistent, long-lived connections (often maintained in a pool) to communicate with storage nodes in various Availability Zones. This approach reduces the overhead associated with establishing a connection repeatedly and helps maintain predictable, low-latency performance even under heavy load.

for id in $(ipcs -m | awk 'NR>3 {print $2}'); do
    ipcrm -m $id
done
for id in $(ipcs -s | awk 'NR>3 {print $2}'); do
    ipcrm -s $id
done


